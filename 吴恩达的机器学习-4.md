---
title: 吴恩达的机器学习-4
date: 2018-09-09 19:46:55
categories: 机器学习
tags: [机器学习,梯度下降算法,多元,正规方程,特征缩放]

---

<Excerpt in index | 首页摘要> 

第四课，主要是讲述多变量线性回归。相对于之前的单变量的区别。还有一个正规方程的使用

<!-- more -->

<The rest of contents | 余下全文>

## 多元梯度下降法演练 I – 特征缩放

如果特征值之间相差太过于大的话，那么代价函数可能是一个非常非常细长的一个椭圆。所以我们需要我们的特征值的范围不要过度的巨大，一般在-3<x<3这样的范围是合理的(这个合理的定义我认为是根据自己来定的，而这个-3到3的范围是吴恩达给我们的一个参考)。如果特征值确实过大，这个时候就需要一个均值归一化的工作了。如下图

![image-20180906155604789](http://www.lzyclay.cn/md_img/均值归一化.png)

图中，x1即为原值。而u1就是整个训练集的x1特征值的平均数。S1是max(x1)-min(x1)。S1其实不用这么的死板固定，只要将特征值缩小即可，并不做过多的约束

## 多元梯度下降法II – 学习率

学习率也就是坡度下降算法中，特征值迭代的过程中的α。当特征值太大的话会导致有代价函数反复的情况，而太小的又会导致学习的效率变低，所以如何选取一个合适的α是至关重要的存在。

![image-20180906155604789](http://www.lzyclay.cn/md_img/学习率1.png)

一般在学习的过程中，都会先描写这样的一个图。该图的横坐标是迭代的次数，而纵坐标是代价函数。可以看出这是一个正常的迭代的过程，随着迭代次数的慢慢变高，代价函数在慢慢的变低，最后趋于稳定。但是如果α选取一个并不合适的值的话，就会就会变成下面这种情况。

![image-20180906155604789](http://www.lzyclay.cn/md_img/学习率2.png)

当代价函数在上升或者呈反复波动的时候，就应该考虑是不是α设置得过大，导致在在一个凹形中往返。这样的情况下，就需要将α变小。在迭代的过程中，我们都可以画这样的一个图去看看学习率是怎么样的一个变化，通过看学习率的变化，来调整α的大小。而一般选择α是可以有一套去选择的，比如三倍三倍的加。0.001-0.003-0.01-0.03-0.1-0.3  这样的变化率来观察学习率的变化，找出最合适的，来进行迭代

## 特征和多项式回归

在选择特征值上，也并是不会固定的。比如下面这个例子：	

![image-20180906155604789](http://www.lzyclay.cn/md_img/多项式回归.png)

当你准备出售你房子的事，你手中的特征值房子的长宽，虽然可以使用这个特征值来建立函数去拟合数据集。但是其实我们可以设一个x为长宽的乘积，即面积。于是我们将面积变成我们新的特征值。

![image-20180906155604789](http://www.lzyclay.cn/md_img/多项式回归1.png)

在构建函数的时候，我们也可以发现不同的对于数据集，我们其实可以有不同的函数可以去拟合的。但是不同的函数拟合的程度是不一样的，可能在一元二次函数的时候，前半段是拟合的，但是在后半段的时候，拟合其实是不对的，不可能面积越高价格越低。所以我们其实可以用一个一元三次函数来拟合，就会发现其实更适合这个数据集。

![image-20180906155604789](http://www.lzyclay.cn/md_img/多项式回归2.png)

再比如这个例子中也是这样的，有时候方程构建的方式并不是唯一的，可以进行多次的尝试。所以我们在处理多个特征值的时候，可以进行适合的处理，让我们的拟合更加的符合数据集。

## 正规方程（区别于迭代方法的直接解法）

正规方程和有迭代的梯度下降区别在于，正规方程可以直接一步出最小值θ。其实原理就是下图的函数的切线原理，先对J(θ)函数求导之后，J(θ)'=0的时候的θ的取值即为最优解。原理也就是微积分的知识这里就不解释了。

![image-20180906155604789](http://www.lzyclay.cn/md_img/正规方程2.png)

然后我们将训练集写成这样的一个矩阵的样子。将其分为了X特征值矩阵，y向量。从而得出了红框的方程，通过这个方程可以直接得出一个最佳的参数，从而构建出最佳的函数，拥有了最低的代价矩阵。

![image-20180906155604789](http://www.lzyclay.cn/md_img/正规方程3.png)

但是正规方程并是不会很完美的存在的。比如当特征值特别多的时候，一个非常大的矩阵转置相乘之后求逆是很费时间的一件事情，但是使用梯度下降算法就很稳定的能处理这办法。一般当特征值大于10000的时候我们就应该使用梯度下降算法了。而且在一些复杂的算法中，正规方程也是不适用的，比如分类算法。

![image-20180906155604789](http://www.lzyclay.cn/md_img/正规方程4.png)

​	