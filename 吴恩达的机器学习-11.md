---
title: 吴恩达的机器学习-11
date: 2018-10-10 15:16:41
categories: 机器学习
tags: [机器学习,应用机器学习]
---

<Excerpt in index | 首页摘要> 

这个章节主要是讲述了如何在实际中去使用应用机器学习的过程。

<!-- more -->

<The rest of contents | 余下全文>

## 评估假设

一旦我们通过以下方式为我们的预测中的错误拍摄了一些问题：

- 获得更多培训示例
- 尝试较小的功能集
- 尝试其他功能
- 尝试多项式特征
- 增加或减少λ

我们可以继续评估我们的新假设。

假设可能对训练样本的误差较小，但仍然不准确（因为过度拟合）。因此，为了评估假设，给定训练样本的数据集，我们可以将数据分成两组：**训练集**和**测试集**。通常，训练集包含70％的数据，测试集包含剩余的30％。

1. 学习theta并且尽量减少训练集代价函数
2. 计算训练集的误差

测试集错误：

1. 对于线性回归 $Jtest(Θ)=\frac{1}{2m_{test}}\sum_{i=1}{m_{test}}(hΘ(x\frac{(i)}{test})−y\frac{(i)}{test})^2$

2. 用于分类〜误分类错误（又名0/1错误分类错误）：

   			1	if $h_\theta$(x)>=0.5 and y=0 or $h_\theta$<0.5 and y=1

   err($h_\theta(x),y$)=

   			0	otherwise

这给出了基于错误分类的二进制0或1错误结果。 测试集的平均测试错误是：

![image-20180906155604789](http://www.lzyclay.cn/md_img/暂时1.png)

这为我们提供了错误分类的测试数据的比例。



## 模型选择和训练,验证,测试集

仅仅因为学习算法很好地适合训练集，这并不意味着它是一个很好的假设。它可能过度适应，因此您对测试集的预测会很差。在您训练参数的数据集上测量的假设误差将低于任何其他数据集的误差。

鉴于许多具有不同多项式度的模型，我们可以使用系统方法来识别“最佳”函数。为了选择假设的模型，您可以测试每个多项式的次数并查看错误结果。

将书记及分解为三组的一种方法是：

* 训练集：60%
* 交叉验证集：20%
* 测试集：20%

我们现在可以使用以下方法为三个不同的集合计算三个单独的错误值：

1. 使用每个多项式度数的训练集优化Θ中的参数。
2. 使用交叉验证集找到具有最小误差的多项式度数d。
3. 使用测试集估计泛化错误 Jtest（theta(d)）（d =多项式的θ，误差较小）;

这样，使用测试集未训练多项式d的程度。



## 诊断偏差与方差

在本节中，我们将研究多项式d的程度与我们假设的欠拟合或过拟合之间的关系。方差，是形容数据分散程度的，算是“无监督的”，客观的指标。偏差，形容数据跟我们期望的中心差得有多远，算是“有监督的”，有人的知识参与的指标。

- 我们需要区分**偏差**或**方差**是否是导致预测不良的问题。
- 高偏差是不合适的，高差异是过度拟合。理想情况下，我们需要找到这两者之间的中庸之道。

随着我们增加多项式的次数d，训练误差将趋于**减小**。

同时，交叉验证错误将趋向于**减少**当我们增加ð到一个点，然后它会**增加**为d的增加，形成的凸曲线。

**高偏差（欠拟合）**：$J_{train}(\theta)$和$J_{CV}(\theta)$将会非常的高，同样$J_{train}(\theta) \approx J_{CV}(\theta)$

**高方差（过拟合）**：$J_{train}(\theta)$将会很低并且$J_{CV}(\theta)$将会大大的优于$J_{train}(\theta)$

如下图所示：

![img](http://www.lzyclay.cn/md_img/高方差1.png)



## 正则化和偏差/方差

注意：正则化术语应该是$\frac {\lambda}{2m}\sum_{j=1}^n\theta_j^2$ 而不是 $\frac {\lambda}{2m}\sum_{j=1}^m\theta_j^2$

![img](http://www.lzyclay.cn/md_img/高方差2.png)

在上图中，我们看到了 λ增加，我们的拟合变得更加僵硬。另一方面，作为λ接近0，我们倾向于过度拟合数据。那么我们如何选择参数呢？λ让它“恰到好处”？为了选择模型和正则化项λ，我们需要：

1. 创建一个lambdas列表（即λ∈{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24}）;
2. 创建一组具有不同度数或任何其他变体的模型。
3. 迭代通过 λs和每个λ通过所有模型来学习一些Th。
4. 使用学习Θ（用λ计算）计算交叉验证误差$J_{CV}(\theta)$ **没有**正则化或λ= 0。
5. 选择在交叉验证集上产生最低错误的最佳组合。
6. 使用最佳组合Θ和λ，将其应用于$J_{test}(\theta)$ 看看它是否具有良好的问题概括性。



## 学习曲线

在极少数数据点比如一个数据点或者两个上训练算法很容易不产生误差，因为我们总能找到一个准确接触那些点数的二次曲线。因此：

- 随着训练集变大，二次函数的误差增加。
- 在某个m或训练集大小之后，错误值将稳定下来。

**高度偏差的体现：**

太小的训练集：$J_{train}(\theta)$变低但是$J_{CV}(\theta)$更高

太大的训练集：$J_{train}(\theta)$和$J_{CV}(\theta)$都会很高并且两者近乎相等

如果遇到了高偏差的，那么增加训练集并不会对训练结果有什么好的帮助

![高方差2](http://www.lzyclay.cn/md_img/学习曲线1.png)

高度方差的体现：

太小的训练集：$J_{train}(\theta)$变低但是$J_{CV}(\theta)$更高

太打的训练集：$J_{train}(\theta)$随着训练集的增大变大，但是$J_{CV}(\theta)$会持续下降并且不稳定，并且CV会大于train，并且差异很大。

如果算法遇到了高方差，那么增加更多的训练集会有所帮助。

![高方差2](http://www.lzyclay.cn/md_img/学习曲线2.png)

## 决定下一步要做什么

我们的决策过程可细分如下：

- **获得更多培训示例：**修复高方差

- **尝试较小的功能集：**修复高方差

- **添加功能：**修复高偏差

- **添加多项式特征：**修复高偏差

- **减少λ：**修复高偏差

- **增加λ：**修正高方差

### **诊断神经网络**

- 具有较少参数的神经网络**易于不适合**。它的**计算成本**也**更低**。
- 具有更多参数的大型神经网络**易于过度拟合**。它的**计算成本**也**很高**。在这种情况下，您可以使用正则化（增加λ）来解决过度拟合问题。

使用单个隐藏层是一个很好的启动默认值。您可以使用交叉验证集在多个隐藏层上训练您的神经网络。然后，您可以选择性能最佳的那个。

**模型复杂性效应：**

- 低阶多项式（低模型复杂度）具有高偏差和低方差。在这种情况下，模型非常一致。
- 高阶多项式（高模型复杂度）非常适合训练数据，测试数据非常差。这些对训练数据的偏差很小，但方差很大。
- 实际上，我们希望选择介于两者之间的模型，这可以很好地推广，但也能很好地拟合数据。



## 优先考虑工作内容

**系统设计实例：**

给定一组电子邮件数据集，我们可以为每封电子邮件构建一个向量。此向量中的每个条目代表一个单词。通过查找数据集中最常用的单词，向量通常包含10,000到50,000个条目。如果要在电子邮件中找到一个单词，我们会将其各自的条目指定为1，否则如果找不到，则该条目将为0.一旦我们准备好所有x向量，我们训练我们的算法，最后，我们可以用它来分类电子邮件是否是垃圾邮件。

![高方差2](http://www.lzyclay.cn/md_img/优先考虑.png)

那么你怎么能花时间提高这个分类器的准确性呢？

- 收集大量数据（例如“蜜罐”项目，但并不总是有效）
- 开发复杂的功能（例如：在垃圾邮件中使用电子邮件标题数据）
- 开发算法以不同方式处理您的输入（识别垃圾邮件中的拼写错误）。

很难说哪个选项最有用。

