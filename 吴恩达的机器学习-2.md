---
title: 吴恩达的机器学习-2
date: 2018-09-06 10:43:57
categories: 机器学习
tags: [机器学习,线性回归,代价函数,梯度算法]
---

<Excerpt in index | 首页摘要> 

今天来到了机器学习的第二课，单变量的线性回归。

<!-- more -->

<The rest of contents | 余下全文>

## 线性回归

在统计学中，线性回归（Linear Regression）是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。

有了一些自变量、因变量的数据，拿一个数学函数Model（模型）去拟合（适配）这些数据，以便之后能根据这个模型，在自变量（条件）给出后，预测因变量的值。

## 代价函数

比如你预测一个1250平方的房子的售价，根据你手上的训练集，定出一个合理的一元函数，然后将这个函数定义为H(x),而1/2m*(H(x)-y)²就是一个代价函数。y代表了实际的售价，m表示训练集数量。这个代价函数的值越小，说明这个预测越准确。

![image-20180906155604789](http://www.lzyclay.cn/md_img/image-20180906155604789.png)

代价函数也被称为平方误差函数，有时也被称为平方误差代价函数。而平方误差函数对于大多数问题(特别是回归问题都)是一个合理的选择。当然也还有其他的代价函数，但是平方误差代价函数可能是最常用的之一了。

### 代价函数的形状

当只有一个参数的时候，代价函数的图形是一个碗状函数，当有两个参数时，也是一个碗状函数，只不过是3D曲面图等高线或等高图像

![image-20180906171816478](http://www.lzyclay.cn/md_img/WX20180906-171718@2x.png)

上图为参数唯一的时候的代价函数J和假设函数H(x)

![image-2018](http://www.lzyclay.cn/md_img/代价函数2.png)

上图为参数有两个的时候的代价函数J和假设函数H(x).

## 梯度下降算法

梯度下降算法不仅用于线性回归算法中，还常用于机器学习其他的场景中。梯度下降算法的目的是计算在代价函数中，找到代价最小的那两个参数是多少，构建最优的假设函数。

简单的来说就是先确定两个初始参数的值A_0和A_1，通常情况下我们先将这两个参数都先设置为0	，然后在之后的工作中不断的一点点提高A_0和A_1的大小，来让代价函数J(A_0,A_1)减少，直到我们将其减少到最小值。

![image-2018](http://www.lzyclay.cn/md_img/梯度算法3.png)

![image-2018](http://www.lzyclay.cn/md_img/梯度算法4.png)

但是由于选择的初始点有所不同，以至于在梯度下降过程中的路程和目的地都有所不同，上两图就表示了在不同的初始点的情况下，不同的路径于不同的终点。

### 梯度算法定义

![image-2018](http://www.lzyclay.cn/md_img/梯度算法算法定义.png)

在梯度算法中只需要重复不停地求最佳参数的过程即可，但是在求两个以上最佳参数的时候，必须同时变化。这样的处理是为了防止其中一个参数变化之后，对其之后的参数进行了坏的影响。接下来我们讨论一下其中的参数α的大小问题。

![image-2018](http://www.lzyclay.cn/md_img/梯度算法5.png)

上述就是将梯度算法的导数运算之后的简化例子。

![image-2018](http://www.lzyclay.cn/md_img/系数太大.png)

在梯度算法中，系数α的大小取值是有要求的。如上图第一种情况所示，如果系数α的大小过小，那么参数下降的幅度就会很小，这样虽然精度上升了，但是对效率有着极大的坏处，浪费了大量的时间。而第二种情况就是α设置得过大，当初始参数在一个较优值时候，因为较大的α会导致直接跨过了最后值，然后又回来，上图第二种情况就显示了一个值点来回跨越的情况。

∂/∂θ1*J(θ1)这个偏导数的实际意义其实就是图中曲线的斜率，当参数渐渐地接近最优参数的位置的时候，参数点的斜率就越来越小，这样变化的步伐就会越来越小了，精确度就会提高，所以没必要要求太小的α。

