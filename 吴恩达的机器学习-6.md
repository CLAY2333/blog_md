---
title: 吴恩达的机器学习-6
date: 2018-09-12 12:50:47
categories: 机器学习
tags: [Logistic,分类算法,机器学习]
---

<Excerpt in index | 首页摘要> 

第6课，主要是讲了Logistic 回归算法。

<!-- more -->

<The rest of contents | 余下全文>

## 分类

要尝试分类，一种方法是使用线性回归并将大于0.5的所有预测映射为1，将所有小于0.5的预测映射为0.但是，此方法不能很好地工作，因为分类实际上不是线性函数。如下图

![image-20180906155604789](http://www.lzyclay.cn/md_img/分类1.png)

所以其实在分类中真正的函数不是这个样子的。分类问题就像回归问题一样，除了我们现在想要预测的值只占用少量离散值。现在，我们将重点关注二进制分类问题，其中y只能处理两个值0和1.（我们在这里所说的大多数也将推广到多类情况。）例如，如果我们正在尝试为电子邮件构建垃圾邮件分类器，然后𝐱（𝒊）可能是一封电子邮件的某些功能，如果是垃圾邮件，则y可能为1，否则为0。
因此，y∈{0,1}。 0也称为负类，1是正类，它们是
有时也用符号“ - ”和“+”表示。给定𝐱（𝒊），相应的𝐲（𝒊）也称为训练样例的标签。	

##假设陈述

我们可以忽略y是离散值的事实来处理分类问题，并使用我们的旧线性回归算法来尝试预测给定x。 但是，很容易构建此方法执行得非常差的示例。 直觉上，当我们知道𝐡θ（x）取大于1或小于0的值时也没有意义
y∈{0,1}。 为了解决这个问题，让我们将假设𝐡θ（x）的形式改为
满足0≤𝐡θ（x）≤1。 这是通过将θ𝑻x插入Logistic函数来实现的。 我们的新形式使用“Sigmoid函数”，也称为“逻辑函数”：

![image-20180906155604789](http://www.lzyclay.cn/md_img/假设1.png)

下图显示了sigmoid函数的样子：

![image-20180906155604789](http://www.lzyclay.cn/md_img/假设2.png)

这里显示的函数g（z）将任何实数映射到（0,1）区间，从而形成它
用于将任意值函数转换为更适合的函数
分类。
𝐡θ（x）将给出我们输出为1的概率。例如，𝐡θ（x）= 0.7给出了我们输出为1的概率为70％。我们的预测为0的概率只是我们的补充 它是1的概率（例如，如果它是1的概率是70％，则它是0的概率是30％）。

##决策界限

为了得到离散的0或1分类，我们可以将假设函数的输出转换如下：

𝐡θ（x）≥0.5→y =1

𝐡θ（x）<0.5→y = 0

我们的逻辑函数g的行为方式是当它的输入大于或等于
零，其输出大于或等于0.5：

g(z)≥0.5
when z≥0

记住了！！！

![image-20180906155604789](http://www.lzyclay.cn/md_img/假设3.png)

所以当你输入的g是𝛉𝑻X的话，那就意味着

![image-20180906155604789](http://www.lzyclay.cn/md_img/假设4.png)

通过这些内容我们可以推断出这样的结果

𝛉𝑻x ≥0⇒y=1
𝛉𝑻x <0⇒y=0

决策边界是分隔y = 0和y = 1的区域的线，它由我们的假设函数创建。

θ= [ 5,-1,0 ] 

y=1 

if 5+(−1) 𝐱𝟏+0 𝐱𝟐≥0		 

5− 𝐱𝟏≥0 

− 𝐱𝟏≥−5 

𝐱𝟏≤5 

在这种情况下，我们的决策边界是放置在图表上的直线垂直线,其中𝐱𝟏= 5，左边的所有东西都表示y = 1，而右边的一切都是表示y = 0.同样,S形函数g(z)的输入(例如θ𝑻X)不需要是线性的,并且可以是描述圆（例如z =θ_0+θ_1𝐱²_1+θ_2𝐱²_1）或任何形状以适合我们的数据的函数。。



## 代价函数

我们不能使用与线性回归相同的成本函数，因为Logistic函数会导致输出波动，从而导致许多局部最优。 换句话说，它不是凸函数。相反我们的逻辑回归成本就是下面这个函数：

![image-20180906155604789](http://www.lzyclay.cn/md_img/代价1.png)

当y=1的时候，我们可以得到以下的J(θ)和hθ(x)的图，可以看出来当函数趋近1的时候，代价函数趋近与0.

![image-20180906155604789](http://www.lzyclay.cn/md_img/代价2.png)

当y=0的时候，我们可以得到以下的J(θ)和hθ(x)的图，同上。	

![image-20180906155604789](http://www.lzyclay.cn/md_img/代价3.png)

![image-20180906155604789](http://www.lzyclay.cn/md_img/代价4.png)

如果我们的正确答案'y'为0，那么如果我们的假设函数也输出0，则成本函数将为0.如果我们的假设接近1，那么成本函数将接近无穷大。
如果我们的正确答案'y'为1，那么如果我们的假设函数输出1，则成本函数将为0.如果我们的假设接近0，那么成本函数将接近无穷大。
请注意，以这种方式编写成本函数可以保证J（θ）对于逻辑回归是凸的。

##简化代价函数与梯度下降

我们可以将上面的那几个函数综合一些变成下面这种函数：

![](http://www.lzyclay.cn/md_img/函数优化1.png)

我们可以看出来，当y为1的时候，后则是(1-y)是等于0的一个算式，所以对最后的结果是不会有结果的。而当y为0的时候，前一个算式整体也为0所以对对后者也没影响了。那我们可以完整地写出我们的整个成本函数如下：

![](http://www.lzyclay.cn/md_img/函数优化2.png)

用矢量化实现是这个样子的：

![](http://www.lzyclay.cn/md_img/函数优化3.png)

请记住，梯度下降的一般形式是：

![](http://www.lzyclay.cn/md_img/函数优化4.png)

我们可以使用微积分来计算导数部分：

![](http://www.lzyclay.cn/md_img/函数优化5.png)

请注意，此算法与我们在线性回归中使用的算法相同。 我们仍然必须同时更新theta中的所有值。 矢量化实现是：

![](http://www.lzyclay.cn/md_img/函数优化6.png)

##高级优化

“共轭梯度”，“BFGS”和“L-BFGS”是更复杂，更快速的优化θ的方法，可用于代替梯度下降。 我们建议您不要自己编写这些更复杂的算法（除非您是数值计算专家），而是使用库，因为它们已经过测试和高度优化。 Octave提供它们。
我们首先需要提供一个函数来评估给定的以下两个函数

##多元分类：一对多

现在，当我们有两个以上的类别时，我们将接近数据分类。 我们将扩展我们的定义，而不是y = {0,1}，而y = {0,1 ... n}。
由于y = {0,1 ... n}，我们将问题分为n + 1（+1，因为索引从0开始）二进制分类问题; 在每一个中，我们预测'y'是我们其中一个类的成员的概率。

![](http://www.lzyclay.cn/md_img/一对多1.png)

我们基本上选择一个类，然后将所有其他类归为一个二级类。 我们反复这样做，对每个案例应用二元逻辑回归，然后使用返回最高值的假设作为我们的预测。下图显示了如何对3个类进行分类：

![](http://www.lzyclay.cn/md_img/一对多2.png)

总结一下：
训练每个类的逻辑回归分类器𝐡θ（𝒙）以预测y = i的概率。要对新x进行预测，请选择最大化𝐡θ（𝒙）的类

